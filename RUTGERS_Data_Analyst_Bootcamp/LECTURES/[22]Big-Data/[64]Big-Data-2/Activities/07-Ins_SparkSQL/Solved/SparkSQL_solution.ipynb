{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crlKd6Qpq6BU",
    "outputId": "595b1bab-4f91-4c19-9eeb-5dfb9f4b5ecd"
   },
   "outputs": [],
   "source": [
    "# Import findspark and initialize. \n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t3vOTJF5rD_4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/08 11:32:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType,StructField,StringType, DateType,IntegerType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-mOHq6oaizia"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+------+--------------------+----------+------------+------+---------+\n",
      "| id|show_id|   type| title|             country|date_added|release_year|rating| duration|\n",
      "+---+-------+-------+------+--------------------+----------+------------+------+---------+\n",
      "|  0|     s1|TV Show|    3%|              Brazil| 14-Aug-20|        2020| TV-MA|4 Seasons|\n",
      "|  1|     s2|  Movie|  7:19|              Mexico| 23-Dec-16|        2016| TV-MA|   93 min|\n",
      "|  2|     s3|  Movie| 23:59|           Singapore| 20-Dec-18|        2011|     R|   78 min|\n",
      "|  3|     s4|  Movie|     9|       United States| 16-Nov-17|        2009| PG-13|   80 min|\n",
      "|  4|     s5|  Movie|    21|       United States|  1-Jan-20|        2008| PG-13|  123 min|\n",
      "|  5|     s6|TV Show|    46|              Turkey|  1-Jul-17|        2016| TV-MA| 1 Season|\n",
      "|  6|     s7|  Movie|   122|               Egypt|  1-Jun-20|        2019| TV-MA|   95 min|\n",
      "|  7|     s8|  Movie|   187|       United States|  1-Nov-19|        1997|     R|  119 min|\n",
      "|  8|     s9|  Movie|   706|               India|  1-Apr-19|        2019| TV-14|  118 min|\n",
      "|  9|    s10|  Movie|  1920|               India| 15-Dec-17|        2008| TV-MA|  143 min|\n",
      "| 10|    s11|  Movie|  1922|       United States| 20-Oct-17|        2017| TV-MA|  103 min|\n",
      "| 11|    s12|TV Show|  1983|Poland, United St...| 30-Nov-18|        2018| TV-MA| 1 Season|\n",
      "| 12|    s13|TV Show|  1994|              Mexico| 17-May-19|        2019| TV-MA| 1 Season|\n",
      "| 13|    s14|  Movie| 2,215|            Thailand|  1-Mar-19|        2018| TV-MA|   89 min|\n",
      "| 14|    s15|  Movie|  3022|       United States| 19-Mar-20|        2019|     R|   91 min|\n",
      "| 15|    s16|  Movie| 1-Oct|             Nigeria|  1-Sep-19|        2014| TV-14|  149 min|\n",
      "| 16|    s17|TV Show| 9-Feb|                null| 20-Mar-19|        2018| TV-14| 1 Season|\n",
      "| 17|    s18|  Movie|22-Jul|Norway, Iceland, ...| 10-Oct-18|        2018|     R|  144 min|\n",
      "| 18|    s19|  Movie|15-Aug|               India| 29-Mar-19|        2019| TV-14|  124 min|\n",
      "| 19|    s20|  Movie|   '89|      United Kingdom| 16-May-18|        2017| TV-PG|   87 min|\n",
      "+---+-------+-------+------+--------------------+----------+------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/2/better_netflix_titles.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"better_netflix_titles.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MZLbpHe-n_MR"
   },
   "outputs": [],
   "source": [
    "# Create our temporary view\n",
    "df.createOrReplaceTempView('movies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ednpJUL9oGTJ",
    "outputId": "1702c674-1a50-4590-e4e0-bb49239ff028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-------------+----------+------------+------+--------+\n",
      "|show_id| type|title|      country|date_added|release_year|rating|duration|\n",
      "+-------+-----+-----+-------------+----------+------------+------+--------+\n",
      "|     s2|Movie| 7:19|       Mexico|      null|        2016| TV-MA|  93 min|\n",
      "|     s3|Movie|23:59|    Singapore|      null|        2011|     R|  78 min|\n",
      "|     s4|Movie|    9|United States|      null|        2009| PG-13|  80 min|\n",
      "|     s5|Movie|   21|United States|      null|        2008| PG-13| 123 min|\n",
      "|     s7|Movie|  122|        Egypt|      null|        2019| TV-MA|  95 min|\n",
      "|     s8|Movie|  187|United States|      null|        1997|     R| 119 min|\n",
      "|     s9|Movie|  706|        India|      null|        2019| TV-14| 118 min|\n",
      "|    s10|Movie| 1920|        India|      null|        2008| TV-MA| 143 min|\n",
      "|    s11|Movie| 1922|United States|      null|        2017| TV-MA| 103 min|\n",
      "|    s14|Movie|2,215|     Thailand|      null|        2018| TV-MA|  89 min|\n",
      "+-------+-----+-----+-------------+----------+------------+------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can perform most any SQL action at this point\n",
    "# here we are converting the date to a more workable date object\n",
    "#NOTE: since we are not assigning this to a dataframe the change is not saved.\n",
    "spark.sql(\"\"\"SELECT show_id, \n",
    "   type, \n",
    "   title, \n",
    "   country, \n",
    "   TO_DATE(date_added, 'MMMM d, yyyy') \n",
    "   AS date_added, \n",
    "   release_year, \n",
    "   rating, \n",
    "   duration \n",
    "   FROM movies \n",
    "   WHERE date_added IS NOT null AND type='Movie'\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVT_qRomo959",
    "outputId": "2cedab4b-e695-4f71-cd16-b294708e007c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|  rating|number_of_ratings|\n",
      "+--------+-----------------+\n",
      "|   TV-MA|             2863|\n",
      "|   TV-14|             1931|\n",
      "|   TV-PG|              805|\n",
      "|       R|              665|\n",
      "|   PG-13|              386|\n",
      "|    TV-Y|              280|\n",
      "|   TV-Y7|              271|\n",
      "|      PG|              247|\n",
      "|    TV-G|              194|\n",
      "|      NR|               84|\n",
      "|       G|               39|\n",
      "|    null|                9|\n",
      "|TV-Y7-FV|                6|\n",
      "|      UR|                5|\n",
      "|   NC-17|                3|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All of the SQL you learned in Unit 6 is available to you in Spark SQL\n",
    "# Here we are listing out the counts by rating\n",
    "# NOTE: it is almost NEVER a good idea to \"order by\" when using Spark with large datasets (more on this in 8.2)\n",
    "spark.sql(\"\"\"\n",
    "  SELECT\n",
    "    rating,\n",
    "    count(*) AS number_of_ratings\n",
    "  FROM movies\n",
    "  GROUP BY rating\n",
    "  ORDER BY 2 DESC\n",
    "  \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1_PVtGyJzJqU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----------+--------+\n",
      "|               title|rating|date_added|duration|\n",
      "+--------------------+------+----------+--------+\n",
      "|                   9| PG-13| 16-Nov-17|  80 min|\n",
      "|                  21| PG-13|  1-Jan-20| 123 min|\n",
      "|            Ã†on Flux| PG-13|  1-Feb-18|  93 min|\n",
      "|         10,000 B.C.| PG-13|  1-Jun-19| 109 min|\n",
      "|           16 Blocks| PG-13|  1-Nov-19| 102 min|\n",
      "|            17 Again| PG-13|  1-Jan-21| 102 min|\n",
      "|20 Feet From Stardom| PG-13| 22-Sep-18|  91 min|\n",
      "|             28 Days| PG-13| 30-Sep-20| 104 min|\n",
      "|      3 Days to Kill| PG-13|  1-Dec-20| 117 min|\n",
      "|       3 Generations| PG-13| 28-Aug-17|  92 min|\n",
      "|            3 Idiots| PG-13|  1-Aug-19| 164 min|\n",
      "|        5 Flights Up| PG-13| 17-Mar-19|  92 min|\n",
      "|      50 First Dates| PG-13|  1-Dec-20|  99 min|\n",
      "|        A 2nd Chance|    PG|  1-Jul-17|  95 min|\n",
      "|     A Boy Called Po|    PG| 15-Jan-18|  94 min|\n",
      "|    A Bridge Too Far|    PG|  1-Jul-20| 176 min|\n",
      "|A California Chri...| PG-13| 14-Dec-20| 107 min|\n",
      "|    A Champion Heart|     G| 14-Apr-20|  90 min|\n",
      "|  A Cinderella Story|    PG|  1-Jan-20|  95 min|\n",
      "|A Cinderella Stor...|    PG|  1-Dec-19|  86 min|\n",
      "+--------------------+------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's output a file with just listing for children\n",
    "# first we will use our spark sql to write to a dataframe\n",
    "\n",
    "out_df= spark.sql(\"\"\"\n",
    "  SELECT \n",
    "  title,\n",
    "  rating,\n",
    "  date_added,\n",
    "  duration\n",
    "  FROM Movies\n",
    "  WHERE rating IN ('G','PG', 'PG-13')\"\"\")\n",
    "\n",
    "# Make sure we got what we wanted\n",
    "out_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "z4csQ38M6xCl"
   },
   "outputs": [],
   "source": [
    "#  As Spark stores the data in partitions, it will also write data in partitions.\n",
    "#  These partitions will always be stored in a folder with the same name as the file, and that folder may often contain many subfolders or files.\n",
    "#  Within the partition folder, there will be a file or files that starts with `part-`, these are CSV files. \n",
    "# However, they are often not optimal for friendly reading, but can be downloaded to your computer.\n",
    "\n",
    "out_df.write.csv('movies_out_spark.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wAbO6VJL65Nb"
   },
   "outputs": [],
   "source": [
    "# The easiest work around of the part file output is to take the data to Pandas and write out a CSV.\n",
    "# This forces the data to the master node and is not recommended unless you have filtered and/or aggregated your data to a reasonable size.\n",
    "\n",
    "out_df.toPandas().to_csv('movies_out_pandas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rn9uNP137FdI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SparkSQL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
